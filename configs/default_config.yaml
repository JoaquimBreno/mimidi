model:
  type: Seq2Seq
  d_model: 256
  d_latent: 256
  n_codes: 256
  n_groups: 4
  context_size: 256
  encoder_layers: 3
  decoder_layers: 3
  num_attention_heads: 4
  intermediate_size: 512
  lr: 0.0001
  lr_schedule: sqrt_decay
  warmup_steps: 1000
  max_steps: 10000

data:
  batch_size: 32
  max_bars: 64
  max_positions: 256

training:
  num_epochs: 10
  log_interval: 100
  save_model: true
  model_save_path: ./models/seq2seq_model.pth

evaluation:
  eval_interval: 500
  eval_metric: perplexity

description:
  flavor: both
  options: 
    - option1
    - option2